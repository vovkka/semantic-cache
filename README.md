# Отчет по реализации семантического кэширования промтов
## Обзор реализации

Данная реализация использует семантическое кэширование для оптимизации вызовов OpenAI API путем сокращения избыточных запросов для семантически похожих промтов. Система работает следующим образом:

1. **Векторное представление**: Каждый промт преобразуется в 1536-мерный вектор с помощью модели text-embedding-ada-002 от OpenAI.
2. **Поиск похожих промтов**: Используется FAISS (Facebook AI Similarity Search) для эффективного поиска похожих промтов в кэше.
3. **Управление кэшем**: Ответы сохраняются вместе с соответствующими векторами промтов для последующего использования.

## Техническая реализация

### Ключевые компоненты

- **FAISS индекс**: Использует L2-расстояние для измерения схожести
- **Порог схожести**: По умолчанию установлен на 0.95 (настраиваемый)
- **Модель эмбеддингов**: OpenAI text-embedding-ada-002
- **Персистентность**: Кэш может быть сохранен на диск и загружен с него

### Анализ использования токенов

Система обменивает токены эмбеддингов на потенциальную экономию токенов промта:

- **Стоимость промаха кэша**: Токены исходного промта + токены эмбединга
- **Стоимость попадания в кэш**: Только токены эмбеддинга
- **Точка окупаемости**: Кэш становится эффективным, когда на вход поступает достаточное количество семантически близких промахов

- **Экономическая эффективность**: 
  - Токены эмбеддингов (text-embedding-ada-002) стоят примерно в 20 раз дешевле, чем токены GPT-3.5/4. Например, 1М токенов эмбеддинга стоят $0.01, тогда как 1М токенов GPT-3.5-turbo стоят $2
  - Существует возможность полностью исключить затраты на эмбеддинги, используя локальные модели, такие как:
    - Sentence-BERT (например, all-MiniLM-L6-v2)
    - FastText
    - Universal Sentence Encoder
  Эти модели работают локально, не требуют API-запросов и могут быть оптимизированы под конкретную задачу

## Анализ эффективности

### Преимущества

1. **Экономия токенов**: Значительная экономия для длинных, повторяющихся промтов
2. **Скорость ответа**: Попадания в кэш возвращают результат мгновенно
3. **Семантическое понимание**: Учитывает смысл, а не только точные совпадения

### Ограничения

1. **Холодный старт**: Первоначальные запросы все равно требуют полного использования токенов
2. **Накладные расходы на эмбеддинги**: Каждый запрос требует вычисления эмбеддинга
3. **Семантический дрейф**: Очень похожие промты могут требовать разных ответов

## Результаты тестирования

Тестирование проводилось (в examples/benchmark) на наборе из 50 промтов, сгенерированных автоматически с помощью PromptGenerator. Для каждого базового промта были созданы семантически похожие вариации. Для рассчета цены используются реальный прайсинг с OpenAI (для gpt3.5-turbo)

### Метрики производительности

```json
{
   "total_prompts": 50, // Total number of prompts processed
   "cache_hits": 19, // Number of prompts that were served from cache
   "tokens_with_cache": 6640, // Total number of tokens processed with cache
   "tokens_without_cache": 9197, // Total number of tokens processed without cache
   "price_with_cache": 0.011727699999999994, // Cost incurred with cache usage
   "price_without_cache": 0.018393999999999997, // Cost incurred without cache usage
   "price_saved_percent": 36.24170925301732, // Percentage of cost saved by using cache
   "avg_response_time": 0.17915353298187256, // Average response time in seconds
   "total_time": 8.957676649093628, // Total time taken for all prompts in seconds
   "cache_hit_rate": 0.38, // Cache hit rate as a fraction
   "avg_prompt_tokens": 16.34, // Average number of tokens per prompt
   "avg_response_tokens": 167.6, // Average number of tokens per response
   "max_prompt_tokens": 26, // Maximum number of tokens in a single prompt
   "max_response_tokens": 220 // Maximum number of tokens in a single response
}
```

### Анализ результатов

1. **Эффективность кэширования**:
   - Достигнуто сокращение использования токенов на 27.8% (с 9197 до 6640 токенов)
   - 38% запросов обслуживались из кэша
   - Среднее время ответа составило 179мс

2. **Экономический эффект**:
   - Прямая экономия составила $0.0067 (с $0.0184 до $0.0117)
   - Процент экономии средств: 36.2%
   - Прогнозируемая экономия на 1М запросов: ~$134

3. **Математическая модель эффективности**:
   (подробный вывод модели в examples/model.pdf)
   - Разработана модель для расчета реальной выгоды с учетом следующих параметров:
     * Количество запросов (n)
     * Процент попаданий в кэш (p)
     * Стоимость токенов эмбеддинга (c_e)
     * Стоимость токенов запроса (c_q)
     * Количество входных токенов (t_input)
     * Количество выходных токенов (t_output)

   - Формула расчета выгоды:
     ```
     profit = n * (p * c_q * (t_input + t_output) - c_e * t_input)
     ```

   - Реальная экономия может существенно отличаться от показанной в эксперименте и зависит от:
     * Длины промптов и ответов
     * Семантической близости запросов
     * Соотношения цен на различные типы токенов
     * Выбранной модели эмбеддингов (OpenAI или локальной)
     * Порога схожести в кэше

### Выводы по результатам тестирования

1. Система особенно эффективна для случаев, когда:
   - Суммарная длинна промпта и ответа нейросети достаточно большая
   - Имеется много семантически похожих запросов
   - Важна скорость ответа


## Рекомендации

1. **Специфика использования**: Наиболее эффективен для приложений с семантически похожими промтами
2. **Настройка порога**: Регулировать порог схожести в зависимости от требуемой точности
3. **Регулярная очистка кэша**: Внедрить ограничения размера кэша и стратегии очистки

## Заключение

Подход с семантическим кэшированием доказывает свою эффективность в конкретных случаях, особенно когда:
- Похожие промты встречаются часто
- Допустима некоторая семантическая гибкость

Реализация обеспечивает хороший баланс между экономией токенов и точностью, с возможностью настройки параметров под конкретные нужды.

## Структура проекта

```
prompt-optimization-research/
├── src/
│   ├── base/
│   │   └── abstract_cache.py     # Абстрактные классы и основная логика кэширования
│   ├── examples/
│   │   ├── benchmark.py         # Инструменты для тестирования производительности
│   │   ├── math_model.py        # Математическая модель расчета эффективности
│   │   └── mock.py             # Мок-клиент для тестирования
│   ├── utils/
│   │   └── prompt_generator.py  # Генератор тестовых промптов
│   ├── local_embeddings.py      # Реализация локального эмбеддинга
│   └── openai_implementations.py # Реализация OpenAI клиентов
├── test_cache.py               # Основной файл для запуска тестов
├── requirements.txt           # Зависимости проекта
└── README.md                 # Документация проекта
```

### Описание основных компонентов

1. **src/base/**
   - `abstract_cache.py`: Содержит базовые абстрактные классы (`EmbeddingModel`, `PromptClient`) и основную реализацию семантического кэша

2. **src/examples/**
   - `benchmark.py`: Инструменты для оценки производительности кэша
   - `math_model.py`: Математическая модель для расчета экономической эффективности
   - `mock.py`: Мок-реализация для тестирования без использования реального API

3. **src/utils/**
   - `prompt_generator.py`: Генератор тестовых промптов с возможностью создания семантически похожих вариаций

4. **Основные реализации**
   - `local_embeddings.py`: Реализация локальных эмбеддингов на базе Sentence Transformers
   - `openai_implementations.py`: Реализации для работы с OpenAI API
